{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91802f7b-2f5c-47b8-b9ce-bebd2e3e78b5",
   "metadata": {},
   "source": [
    "# Dynamic Web scraper\n",
    "\n",
    "This notebook scrapes the MSU courses website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3446b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load selinimum and automatically install the Chrome Driver\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from getpass import getpass\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79409084-ca62-4557-b055-6666e71f0dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chrome Driver\n",
    "# options = webdriver.ChromeOptions()\n",
    "# driver = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2164d-feef-4acd-9efa-687cb506c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FireFox Driver\n",
    "driver = webdriver.Firefox()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0849026d-dde8-44cf-a325-143c1e1ce57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://student.msu.edu/psc/public/EMPLOYEE/SA/c/NUI_FRAMEWORK.PT_AGSTARTPAGE_NUI.GBL?CONTEXTIDPARAMS=TEMPLATE_ID%3aPTPPNAVCOL&scname=MSU_AA_SCHEDULE_NEW0&PanelCollapsible=Y\"\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "body = driver.page_source\n",
    "soup = BeautifulSoup(body, 'html.parser')\n",
    "cells = soup.find_all(\"tr\", class_=\"ps_grid-row psc_rowact\")\n",
    "semesters = dict()\n",
    "for cell in cells: # Find each semester's ID\n",
    "    semester = cell.find(\"a\", class_=\"ps-link\")\n",
    "    semesters[semester.text] = semester.get(\"href\")\n",
    "\n",
    "url = semesters[\"Spring Semester 2024\"]\n",
    "driver.execute_script(url); \n",
    "time.sleep(5)\n",
    "\n",
    "element = driver.find_element(By.ID, 'MSU_CLSRCH_WRK2_SUBJECT')  \n",
    "element.send_keys(\"CMSE\") #pick cmse for example\n",
    "\n",
    "url = f\"javascript:submitAction_win0(document.win0,'MSU_CLSRCH_WRK_SSR_PB_SEARCH');\"\n",
    "driver.execute_script(url); # Hit search\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4ecc2-aa1f-4fd3-9519-ef8ec8d69536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_info(soup):\n",
    "    '''Scrape entire page for all of the class boxes'''\n",
    "    # get the number of results\n",
    "    result_element = soup.find('span', id='MSU_RSLT_NAV_WK_PTPG_ROWS_GRID')\n",
    "    # Extract the text content\n",
    "    result_text = result_element.get_text(strip=True)\n",
    "    result = int(result_text.split()[0])\n",
    "    pages = (result + 49) // 50 # get the number of aggregated pages\n",
    "    \n",
    "    reshaped_list = []\n",
    "    cells = soup.find_all(\"tr\", class_=\"ps_grid-row psc_rowact\") # Find all table row in the page (Classes)\n",
    "    if pages > 1: # If there's more than 1 page, go to the next page and scrape\n",
    "        for i in range(pages - 1):\n",
    "            url = f\"javascript:submitAction_win0(document.win0,'MSU_RSLT_NAV_WK_SEARCH_CONDITION2$46$');\"\n",
    "            driver.execute_script(url)\n",
    "            time.sleep(5)\n",
    "            body = driver.page_source\n",
    "            next_soup = BeautifulSoup(body, 'html.parser')\n",
    "            cells += next_soup.find_all(\"tr\", class_=\"ps_grid-row psc_rowact\")\n",
    "    \n",
    "    for cell in cells:\n",
    "        values = cell.text.split(\"\\n\") # Split text in a cell \n",
    "        values = list(filter(lambda x: x != \"\", values))  \n",
    "\n",
    "        reshaped_list.append(values[:-3]) # Other columns will be scrape from SIS\n",
    "    col_names = ['Course', 'Type', 'Section']\n",
    "    df = pd.DataFrame(reshaped_list, columns=col_names)\n",
    "    return df\n",
    "        \n",
    "body = driver.page_source\n",
    "soup = BeautifulSoup(body, 'html.parser')\n",
    "df = get_basic_info(soup)  # getting info on the first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5589cb6-d5ef-496c-8aa9-e2cd90e518b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://student.msu.edu/splash.html\"\n",
    "driver.get(url)\n",
    "\n",
    "element = driver.find_element(By.ID, 'loginUrl1') # Hit the login button\n",
    "element.click() \n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb9bbe0-7467-4e6e-880b-eeb692019df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signing in\n",
    "element = driver.find_element(By.ID, 'input28') # Find the MSU email input\n",
    "if not element.get_attribute('value'):\n",
    "    print(\"Please enter your MSU email: \")\n",
    "    email = input()\n",
    "    element.send_keys(email)\n",
    "    \n",
    "element = driver.find_element(By.ID, 'input36') # Find the password input\n",
    "print(\"Please enter your password: \")\n",
    "password = getpass()\n",
    "element.send_keys(password)\n",
    "\n",
    "element = driver.find_element(By.CLASS_NAME, 'o-form-button-bar') # Click Sign in\n",
    "element.click()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438f207d-d58c-4e3c-87eb-dbdd1422ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the authentication method by phone, might not be in the right window\n",
    "# If not in the right window, click 'Verify with something else'\n",
    "# If there's an error, run the cell again\n",
    "buttons = driver.find_elements(By.CLASS_NAME, 'authenticator-button')\n",
    "for button in buttons:\n",
    "    if button.get_attribute('data-se') == 'phone_number':\n",
    "        button.click() # Click on authenticate by phone\n",
    "time.sleep(5)\n",
    "\n",
    "element = driver.find_element(By.TAG_NAME, 'input')  \n",
    "element.click() # Click on Receive code via SMS\n",
    "time.sleep(5)\n",
    "\n",
    "element = driver.find_element(By.TAG_NAME, \"input\") # Find the code input\n",
    "print(\"Please enter your SMS code: \")\n",
    "code = input()\n",
    "element.send_keys(code)\n",
    " \n",
    "element = driver.find_element(By.CLASS_NAME, 'o-form-button-bar') # Click Verify\n",
    "element.click()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b44a17-becb-4415-a2b1-b9aef0c6b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    element = driver.find_element(By.ID, \"win0groupletPTNUI_LAND_REC_GROUPLET$1\") # Click on Classes\n",
    "    element.click()\n",
    "    time.sleep(5)\n",
    "except: # In case the website want to login again\n",
    "    element = driver.find_element(By.ID, 'loginUrl1') # Click on Login\n",
    "    element.click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    element = driver.find_element(By.ID, \"win0groupletPTNUI_LAND_REC_GROUPLET$1\")  # Click on Classes\n",
    "    element.click()\n",
    "    time.sleep(5)\n",
    "\n",
    "try:\n",
    "    element = driver.find_element(By.ID, 'SCC_LO_FL_WRK_SCC_VIEW_BTN$2')  # Click on Class Search & Enroll\n",
    "    element.click()\n",
    "    time.sleep(5)\n",
    "except:\n",
    "    pass # If there's no Class Search & Enroll, proceed\n",
    "\n",
    "body = driver.page_source\n",
    "soup = BeautifulSoup(body, 'html.parser')\n",
    "cells = soup.find_all(\"tr\", class_=\"ps_grid-row psc_rowact\")\n",
    "semesters = dict()\n",
    "for cell in cells: # Find each semester's ID\n",
    "    semester = cell.find(\"a\", class_=\"ps-link\")\n",
    "    semesters[semester.text] = semester.get(\"href\")\n",
    "\n",
    "url = semesters[\"Spring Semester 2024\"]\n",
    "driver.execute_script(url);\n",
    "time.sleep(5)\n",
    "\n",
    "element = driver.find_element(By.ID, 'MSU_CLSRCH_WRK2_SUBJECT')  \n",
    "element.send_keys(\"CMSE\") #pick cmse for example\n",
    "\n",
    "url = f\"javascript:submitAction_win9(document.win9,'MSU_CLSRCH_WRK_SSR_PB_SEARCH');\"\n",
    "driver.execute_script(url); # Hit search\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be9395",
   "metadata": {},
   "source": [
    "Function to get basic classes' info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ba5483-1d23-4fbd-a58e-de06009702bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = driver.page_source\n",
    "soup = BeautifulSoup(body, 'html.parser')\n",
    "result_element = soup.find('span', id='MSU_RSLT_NAV_WK_PTPG_ROWS_GRID')\n",
    "# Extract the text content\n",
    "result_text = result_element.get_text(strip=True)\n",
    "result = int(result_text.split()[0])\n",
    "pages = (result + 49) // 50 # get the number of aggregated pages\n",
    "\n",
    "def get_advanced_info(soup):\n",
    "    schedule_ls = []\n",
    "    for i in range(3): # Find the class schedules, there might be multiple\n",
    "        div = soup.find(\"div\", id=f\"win9divMSU_CLS_DTL_WK2_HTMLAREA1$160$${i}\")\n",
    "        if not div:\n",
    "            break\n",
    "        schedule_ls.append(div)\n",
    "        \n",
    "    date_tds = soup.find_all(\"td\", class_=\"ps_grid-cell E_HTMLAREA2\") # Find class dates\n",
    "    loc_ins_tds = soup.find_all(\"td\", class_=\"ps_grid-cell E_HTMLAREA3\") # Find class locations, instructors, and modes\n",
    "\n",
    "    if not loc_ins_tds: # If these cannot be found, return all None\n",
    "        return None, None, None, None, None, None\n",
    "    \n",
    "    schedules = []\n",
    "    dates = []\n",
    "    locs = []\n",
    "    modes = []\n",
    "    names = []\n",
    "    emails = []\n",
    "    for i in range(len(loc_ins_tds)):\n",
    "        # Append multiple schedules to a list\n",
    "        schedule_div = schedule_ls[i].find('div', class_=\"ps-htmlarea\")\n",
    "        schedules.append(schedule_div.get_text(strip=True))\n",
    "        \n",
    "        # Append multiple dates to a list\n",
    "        date_div = date_tds[i].find('div', class_=\"ps-htmlarea\")\n",
    "        dates.append(date_div.get_text(strip=True))\n",
    "\n",
    "        # Append multiple locations and modes to lists\n",
    "        loc_ins_div = loc_ins_tds[i].find('div', class_=\"ps-htmlarea\")\n",
    "        txt = loc_ins_div.get_text(strip=True, separator='\\n')\n",
    "        locs.append(txt.split('\\n')[0])\n",
    "        modes.append(txt.split('\\n')[-1])\n",
    "\n",
    "        # Append multiple professors in different schedules to lists\n",
    "        a_elements = loc_ins_div.find_all(\"a\")\n",
    "        if not a_elements:\n",
    "            continue\n",
    "        email_ind = ''\n",
    "        name_ind = ''\n",
    "        for a_element in a_elements: # If one schedule of the class has more than 1 professor, separate with '\\n'\n",
    "            # Extract the email address from the href attribute\n",
    "            email_ind += a_element['href'].split(':')[1] + '\\n'\n",
    "            name_ind += a_element.text + '\\n'\n",
    "        emails.append(email_ind.strip())\n",
    "        names.append(name_ind.strip())\n",
    "        \n",
    "    return schedules, dates, locs, modes, names, emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeada9c-8702-49f7-ae4c-54ff60a2e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_info(df):\n",
    "    '''Function to add info from the breakout windows for each course\n",
    "    '''\n",
    "    schedules = []\n",
    "    dates = []\n",
    "    locations = []\n",
    "    modes = []\n",
    "    emails = []\n",
    "    names = []\n",
    "    for i in range(pages): # Go through all pages\n",
    "        for i in range(50): # Go through all cells in a page\n",
    "            try:\n",
    "                rowname = f\"DESCR100$0_row_{i}\"\n",
    "                element = driver.find_element(By.ID, rowname) \n",
    "                print(rowname)\n",
    "            except: # End of dataframe, if cannot found, break the loop\n",
    "                break\n",
    "            driver.execute_script(\"arguments[0].click();\", element) # Click on cell\n",
    "            time.sleep(3) # If too slow can change this line, if there's an error, increase the time\n",
    "            driver.switch_to.frame(0) # Switch to Class Description frame\n",
    "            body = driver.page_source\n",
    "            soup = BeautifulSoup(body, 'html.parser')\n",
    "            schedule, date, loc, mode, name, email = get_advanced_info(soup)\n",
    "            schedules.append(schedule)\n",
    "            dates.append(date)\n",
    "            locations.append(loc)\n",
    "            modes.append(mode)\n",
    "            emails.append(email)\n",
    "            names.append(name)\n",
    "            cancel_cmd=\"javascript:doUpdateParent(document.win9,'#ICCancel');\" \n",
    "            driver.execute_script(cancel_cmd); # Close the frame\n",
    "            driver.switch_to.default_content(); # Switch to the main page\n",
    "            time.sleep(2)\n",
    "        \n",
    "        if pages-1 != i: # If there's more pages, click Next\n",
    "            url = f\"javascript:submitAction_win9(document.win9,'MSU_RSLT_NAV_WK_SEARCH_CONDITION2$46$');\"\n",
    "            driver.execute_script(url)\n",
    "            time.sleep(5)\n",
    "        else: # If there's no more page, break out of the big loop\n",
    "            break\n",
    "    \n",
    "    df['Schedule'] = schedules\n",
    "    df['Dates'] = dates\n",
    "    df['Location'] = locations\n",
    "    df['Mode'] = modes\n",
    "    df['email'] = emails\n",
    "    df['Instructor'] = names\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09066f87-e02d-4bba-8ef0-366647ca618b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = add_info(df)\n",
    "df.to_csv(\"Spring2024.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6446fc-5b56-417c-8fa2-39183dc190f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_courses_df(df): \n",
    "    # Explode all these columns, will result in duplicate rows for classes that have multiple schedules\n",
    "    df = df.explode(['Schedule', 'Dates', 'Location', 'Mode', 'email', 'Instructor'])\n",
    "            \n",
    "    df[['Days', 'Time']] = df['Schedule'].str.split(':',n=1,expand=True)\n",
    "    df[['Course Code', 'Course Name']] = df['Course'].str.split(':', n=1, expand=True)  \n",
    "    split_result = df['Type'].str.split('(', n=1, expand=True)\n",
    "    # Check if the split operation resulted in two columns\n",
    "    if len(split_result.columns) == 2:\n",
    "        df[['Type', 'Units']] = split_result\n",
    "    else:\n",
    "        # Handle the case where the split didn't result in two columns\n",
    "        df['Type'] = split_result[0]  # Assign the first part to 'Type'\n",
    "        df['Units'] = '' \n",
    "    df[['Section', 'Class Nbr', 'Academic Session']] = df['Section'].str.split('/', n=2, expand=True)\n",
    "    df[['Units','Status']] = df['Units'].str.split(')',n=1,expand=True)\n",
    "    df[['Subject','Course Number']] = df['Course Code'].str.split(' ',n=1,expand=True)\n",
    "    df['Dates'] = df['Dates'].apply(lambda x: x.replace(\"Approval Required\", '').strip() if x else x)\n",
    "    df['Status'] = df['Status'].str.replace('Reserved Capacity', '').str.strip()\n",
    "    \n",
    "    #df['Dates'] = df['Dates'].str.replace('Approval Required', '').str.strip()\n",
    "    df['Course Name'] = df['Course Name'].str.replace('Cross-Listed', '').str.strip()\n",
    "    df['Course Name'] = df['Course Name'].str.replace('Approval Required', '').str.strip()\n",
    "\n",
    "    df = df.drop(['Course', 'Schedule','Course Code'], axis=1)\n",
    "    df = df[['Subject','Course Number','Course Name','Type','Units','Status',\n",
    "             'Section','Class Nbr','Academic Session','Days','Time','Dates', \n",
    "             'Location', 'Mode', 'email', 'Instructor']]\n",
    "    df['Units'] = df['Units'].str.replace(' units', '')\n",
    "    df['Section'] = df['Section'].str.extract(r'(\\d+(?:\\.\\d+)?)')\n",
    "    df['Class Nbr'] = df['Class Nbr'].str.extract(r'(\\d+(?:\\.\\d+)?)')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4efc555-5b9d-46e6-a967-83e91ae2cb58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = make_courses_df(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b4f191-b654-46e2-bf26-7a982c1bc47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Spring2024.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
